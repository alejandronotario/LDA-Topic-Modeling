{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA TOPIC MODELING\n",
    "\n",
    "This notebook applies LDA modeling to a dataset of news headlines:\n",
    "\n",
    "https://www.kaggle.com/therohk/million-headlines\n",
    "\n",
    "run at Dataproc with pySpark:\n",
    "\n",
    "https://cloud.google.com/dataproc/\n",
    "\n",
    "and following this post:\n",
    "\n",
    "https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e\n",
    "\n",
    "and Apache Spark Documentation:\n",
    "\n",
    "- http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA\n",
    "\n",
    "- https://spark.apache.org/docs/2.1.0/ml-clustering.html#latent-dirichlet-allocation-lda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from nltk.corpus import stopwords\n",
    "import re as re\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "#from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, struct\n",
    "import pyspark.sql.types as T \n",
    "import string\n",
    "import nltk\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.166.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"gs://ei-db/abcnews-date-text.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103665"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|       headline_text|\n",
      "+------------+--------------------+\n",
      "|    20030219|aba decides again...|\n",
      "|    20030219|act fire witnesse...|\n",
      "|    20030219|a g calls for inf...|\n",
      "|    20030219|air nz staff in a...|\n",
      "|    20030219|air nz strike to ...|\n",
      "|    20030219|ambitious olsson ...|\n",
      "|    20030219|antic delighted w...|\n",
      "|    20030219|aussie qualifier ...|\n",
      "|    20030219|aust addresses un...|\n",
      "|    20030219|australia is lock...|\n",
      "|    20030219|australia to cont...|\n",
      "|    20030219|barca take record...|\n",
      "|    20030219|bathhouse plans m...|\n",
      "|    20030219|big hopes for lau...|\n",
      "|    20030219|big plan to boost...|\n",
      "|    20030219|blizzard buries u...|\n",
      "|    20030219|brigadier dismiss...|\n",
      "|    20030219|british combat tr...|\n",
      "|    20030219|bryant leads lake...|\n",
      "|    20030219|bushfire victims ...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transforme pagTitle column to RDD to work with__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=data.rdd.map(lambda x: x['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba decides against community broadcasting licence',\n",
       " 'act fire witnesses must be aware of defamation',\n",
       " 'a g calls for infrastructure protection summit',\n",
       " 'air nz staff in aust strike for pay rise',\n",
       " 'air nz strike to affect australian travellers']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stopwords list, imported from NLTK library__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')#must be downloaded to run\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizing__\n",
    "\n",
    "Filtering the words removing extra spaces between words, split each review into list of words, change them into lowercase, check if they’re alpha numeric, remove any words or typos which are less than three letters, remove any stopwords, and finally add an index to the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = texts \\\n",
    "    .map( lambda document: document.strip().lower()) \\\n",
    "    .map( lambda document: re.split(\" \", document)) \\\n",
    "    .map( lambda word: [x for x in word if x.isalpha()]) \\\n",
    "    .map( lambda word: [x for x in word if len(x) > 3] ) \\\n",
    "    .map( lambda word: [x for x in word if x not in stopwords]) \\\n",
    "    .zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       list_of_words|index|\n",
      "+--------------------+-----+\n",
      "|[decides, communi...|    0|\n",
      "|[fire, witnesses,...|    1|\n",
      "|[calls, infrastru...|    2|\n",
      "|[staff, aust, str...|    3|\n",
      "|[strike, affect, ...|    4|\n",
      "|[ambitious, olsso...|    5|\n",
      "|[antic, delighted...|    6|\n",
      "|[aussie, qualifie...|    7|\n",
      "|[aust, addresses,...|    8|\n",
      "|[australia, locke...|    9|\n",
      "|[australia, contr...|   10|\n",
      "|[barca, take, rec...|   11|\n",
      "|[bathhouse, plans...|   12|\n",
      "|[hopes, launcesto...|   13|\n",
      "|[plan, boost, par...|   14|\n",
      "|[blizzard, buries...|   15|\n",
      "|[brigadier, dismi...|   16|\n",
      "|[british, combat,...|   17|\n",
      "|[bryant, leads, l...|   18|\n",
      "|[bushfire, victim...|   19|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_txts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TF-IDF Matrix__\n",
    "\n",
    "Transforming the rdd into a DataFrame which has two columns — one has index and the other the list of words. CountVectorizer takes this data and returns a sparse matrix of term frequencies attached to the original Dataframe. Same thing goes for the IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF\n",
    "#VocabSize of 20000 words and words with frequencies above 10\n",
    "cv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\",vocabSize=20000, minDF=10.0)\n",
    "cvmodel = cv.fit(df_txts)\n",
    "\n",
    "result_cv = cvmodel.transform(df_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       list_of_words|index|        raw_features|            features|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|[decides, communi...|    0|(20000,[122,1122,...|(20000,[122,1122,...|\n",
      "|[fire, witnesses,...|    1|(20000,[6,585,112...|(20000,[6,585,112...|\n",
      "|[calls, infrastru...|    2|(20000,[21,617,10...|(20000,[21,617,10...|\n",
      "|[staff, aust, str...|    3|(20000,[75,187,21...|(20000,[75,187,21...|\n",
      "|[strike, affect, ...|    4|(20000,[14,187,16...|(20000,[14,187,16...|\n",
      "|[ambitious, olsso...|    5|(20000,[40,1675,1...|(20000,[40,1675,1...|\n",
      "|[antic, delighted...|    6|(20000,[99,1911,2...|(20000,[99,1911,2...|\n",
      "|[aussie, qualifie...|    7|(20000,[166,209,4...|(20000,[166,209,4...|\n",
      "|[aust, addresses,...|    8|(20000,[4,78,151,...|(20000,[4,78,151,...|\n",
      "|[australia, locke...|    9|(20000,[7,3054,48...|(20000,[7,3054,48...|\n",
      "|[australia, contr...|   10|(20000,[7,78,451,...|(20000,[7,78,451,...|\n",
      "|[barca, take, rec...|   11|(20000,[94,99,921...|(20000,[94,99,921...|\n",
      "|[bathhouse, plans...|   12|(20000,[66,149,23...|(20000,[66,149,23...|\n",
      "|[hopes, launcesto...|   13|(20000,[214,1560,...|(20000,[214,1560,...|\n",
      "|[plan, boost, par...|   14|(20000,[8,9,41,23...|(20000,[8,9,41,23...|\n",
      "|[blizzard, buries...|   15|(20000,[529,897,2...|(20000,[529,897,2...|\n",
      "|[brigadier, dismi...|   16|(20000,[368,373,1...|(20000,[368,373,1...|\n",
      "|[british, combat,...|   17|(20000,[357,373,2...|(20000,[357,373,2...|\n",
      "|[bryant, leads, l...|   18|(20000,[407,444,6...|(20000,[407,444,6...|\n",
      "|[bushfire, victim...|   19|(20000,[28,202,20...|(20000,[28,202,20...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_tfidf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataframe to model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model=result_tfidf.select('index','list_of_words','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|index|       list_of_words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[decides, communi...|(20000,[122,1122,...|\n",
      "|    1|[fire, witnesses,...|(20000,[6,585,112...|\n",
      "|    2|[calls, infrastru...|(20000,[21,617,10...|\n",
      "|    3|[staff, aust, str...|(20000,[75,187,21...|\n",
      "|    4|[strike, affect, ...|(20000,[14,187,16...|\n",
      "|    5|[ambitious, olsso...|(20000,[40,1675,1...|\n",
      "|    6|[antic, delighted...|(20000,[99,1911,2...|\n",
      "|    7|[aussie, qualifie...|(20000,[166,209,4...|\n",
      "|    8|[aust, addresses,...|(20000,[4,78,151,...|\n",
      "|    9|[australia, locke...|(20000,[7,3054,48...|\n",
      "|   10|[australia, contr...|(20000,[7,78,451,...|\n",
      "|   11|[barca, take, rec...|(20000,[94,99,921...|\n",
      "|   12|[bathhouse, plans...|(20000,[66,149,23...|\n",
      "|   13|[hopes, launcesto...|(20000,[214,1560,...|\n",
      "|   14|[plan, boost, par...|(20000,[8,9,41,23...|\n",
      "|   15|[blizzard, buries...|(20000,[529,897,2...|\n",
      "|   16|[brigadier, dismi...|(20000,[368,373,1...|\n",
      "|   17|[british, combat,...|(20000,[357,373,2...|\n",
      "|   18|[bryant, leads, l...|(20000,[407,444,6...|\n",
      "|   19|[bushfire, victim...|(20000,[28,202,20...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Defining the model__\n",
    "\n",
    "Here it is defined number of topics (k) and max iterations (maxIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=40\n",
    "max_iterations=50\n",
    "lda_model = LDA(k=num_topics, maxIter=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running the model__\n",
    "\n",
    "Takes a quite long time, depending on the workers and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lda_model.fit(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It can be checked its structure using some commands__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.isDistributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model description__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[90, 137, 119, 15...|[0.02632232113767...|\n",
      "|    1|[102, 6, 166, 278...|[0.02838041859878...|\n",
      "|    2|[120, 179, 191, 2...|[0.02522239735892...|\n",
      "|    3|[22, 62, 44, 75, ...|[0.04394980978980...|\n",
      "|    4|[57, 63, 176, 221...|[0.03466358342920...|\n",
      "|    5|[10, 38, 150, 82,...|[0.03785823626888...|\n",
      "|    6|[76, 114, 188, 20...|[0.03493538183775...|\n",
      "|    7|[4, 16, 86, 68, 164]|[0.05356878726660...|\n",
      "|    8|[13, 17, 46, 98, ...|[0.04449739467222...|\n",
      "|    9| [29, 32, 23, 0, 89]|[0.03625447607374...|\n",
      "|   10|[31, 118, 172, 18...|[0.04255495142358...|\n",
      "|   11|[27, 74, 131, 142...|[0.04392949045335...|\n",
      "|   12|[35, 87, 161, 232...|[0.03886320030379...|\n",
      "|   13|[56, 165, 189, 24...|[0.03765557595745...|\n",
      "|   14|[55, 24, 296, 362...|[0.04443694874912...|\n",
      "|   15|[149, 363, 426, 3...|[0.02509242763643...|\n",
      "|   16|[40, 110, 30, 376...|[0.04050425088939...|\n",
      "|   17|[5, 51, 113, 133,...|[0.06074761194777...|\n",
      "|   18|[107, 83, 71, 43,...|[0.02907684973632...|\n",
      "|   19|[26, 33, 64, 70, ...|[0.04002081040285...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.describeTopics(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(topic=0, termIndices=[90, 137, 119, 155, 170, 168, 187, 197, 202, 134], termWeights=[0.026322321137677046, 0.02236724290301282, 0.021793873579204585, 0.021321143954683378, 0.019888977336663942, 0.01952432602468721, 0.019457027836684716, 0.018889900458627013, 0.01825923098918701, 0.01764186447082078])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.describeTopics().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(20000, 40, [2151.9187, 4402.0983, 5805.5954, 246.8059, 1635.5771, 2.133, 874.6418, 478.9826, ..., 0.258, 0.2558, 0.2896, 86.7744, 0.2288, 0.2195, 0.2577, 0.3562], 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting the dataframe with index and topics an weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|index|       list_of_words|            features|   topicDistribution|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|[decides, communi...|(20000,[122,1122,...|[7.87046718762269...|\n",
      "|    1|[fire, witnesses,...|(20000,[6,585,112...|[7.15877969844703...|\n",
      "|    2|[calls, infrastru...|(20000,[21,617,10...|[0.51560511134553...|\n",
      "|    3|[staff, aust, str...|(20000,[75,187,21...|[0.25712201845998...|\n",
      "|    4|[strike, affect, ...|(20000,[14,187,16...|[0.22051273417340...|\n",
      "|    5|[ambitious, olsso...|(20000,[40,1675,1...|[8.35689422867455...|\n",
      "|    6|[antic, delighted...|(20000,[99,1911,2...|[8.14157283724233...|\n",
      "|    7|[aussie, qualifie...|(20000,[166,209,4...|[4.62855558780362...|\n",
      "|    8|[aust, addresses,...|(20000,[4,78,151,...|[8.48765740543258...|\n",
      "|    9|[australia, locke...|(20000,[7,3054,48...|[0.00113862618909...|\n",
      "|   10|[australia, contr...|(20000,[7,78,451,...|[9.60202707624237...|\n",
      "|   11|[barca, take, rec...|(20000,[94,99,921...|[5.85001139057186...|\n",
      "|   12|[bathhouse, plans...|(20000,[66,149,23...|[8.98043712659164...|\n",
      "|   13|[hopes, launcesto...|(20000,[214,1560,...|[0.25276164027740...|\n",
      "|   14|[plan, boost, par...|(20000,[8,9,41,23...|[7.63218161724862...|\n",
      "|   15|[blizzard, buries...|(20000,[529,897,2...|[6.18318020093406...|\n",
      "|   16|[brigadier, dismi...|(20000,[368,373,1...|[6.19266830021453...|\n",
      "|   17|[british, combat,...|(20000,[357,373,2...|[5.22083088532689...|\n",
      "|   18|[bryant, leads, l...|(20000,[407,444,6...|[6.10476647002144...|\n",
      "|   19|[bushfire, victim...|(20000,[28,202,20...|[0.39348097161257...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(index=0, list_of_words=['decides', 'community', 'broadcasting', 'licence'], features=SparseVector(20000, {122: 5.495, 1122: 7.064, 5537: 8.9797, 10076: 9.9439}), topicDistribution=DenseVector([0.0008, 0.0008, 0.0008, 0.2188, 0.2803, 0.0008, 0.0008, 0.1652, 0.308, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0007, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0007, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mergin dataframes to get the topics and weights form each headline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding index to data dataframe\n",
    "headlines=texts.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "df_headlines = sqlContext.createDataFrame(headlines, [\"headlines\",'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           headlines|index|\n",
      "+--------------------+-----+\n",
      "|aba decides again...|    0|\n",
      "|act fire witnesse...|    1|\n",
      "|a g calls for inf...|    2|\n",
      "|air nz staff in a...|    3|\n",
      "|air nz strike to ...|    4|\n",
      "|ambitious olsson ...|    5|\n",
      "|antic delighted w...|    6|\n",
      "|aussie qualifier ...|    7|\n",
      "|aust addresses un...|    8|\n",
      "|australia is lock...|    9|\n",
      "|australia to cont...|   10|\n",
      "|barca take record...|   11|\n",
      "|bathhouse plans m...|   12|\n",
      "|big hopes for lau...|   13|\n",
      "|big plan to boost...|   14|\n",
      "|blizzard buries u...|   15|\n",
      "|brigadier dismiss...|   16|\n",
      "|british combat tr...|   17|\n",
      "|bryant leads lake...|   18|\n",
      "|bushfire victims ...|   19|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_headlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=df_headlines.join(transformed, on=['index'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|           headlines|       list_of_words|            features|   topicDistribution|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|   26|commonwealth bank...|[commonwealth, ba...|(20000,[30,142,22...|[5.48684415763207...|\n",
      "|   29|councillor to con...|[councillor, cont...|(20000,[848,1214,...|[8.54094794320740...|\n",
      "|  474|bracks backs ruli...|[bracks, backs, r...|(20000,[65,107,10...|[5.91045684361068...|\n",
      "|  964|all ords jumps 33...|[ords, jumps, poi...|(20000,[622,1358,...|[8.08410458255682...|\n",
      "| 1677|tour of valencia ...|[tour, valencia, ...|(20000,[33,177,62...|[7.39797624391325...|\n",
      "| 1697|victorian silk fa...|[victorian, silk,...|(20000,[3,25,138,...|[6.30737653783935...|\n",
      "| 1806|jail for firefigh...|[jail, firefighte...|(20000,[128,2952,...|[8.11545131403488...|\n",
      "| 1950|brogden would eas...|[brogden, would, ...|(20000,[281,298,8...|[0.16213097199264...|\n",
      "| 2040|man dead after fr...|[dead, freak, cri...|(20000,[69,278,41...|[7.54646551948714...|\n",
      "| 2214|cairns hot spot w...|[cairns, spot, wi...|(20000,[40,65,390...|[5.84537752505672...|\n",
      "| 2250|gps consider char...|[consider, chargi...|(20000,[449,1394,...|[0.00106152311511...|\n",
      "| 2453|new rural doctor ...|[rural, doctor, t...|(20000,[34,410,42...|[7.18514251814205...|\n",
      "| 2509|turkish cypriot l...|[turkish, cypriot...|(20000,[253,602,8...|[6.58385508008494...|\n",
      "| 2529|air traffic contr...|[traffic, control...|(20000,[61,359,89...|[4.68642319178847...|\n",
      "| 2927|stevens cleared t...|[stevens, cleared...|(20000,[290,717,9...|[8.75079180926292...|\n",
      "| 3091|mp calls for seco...|[calls, second, k...|(20000,[21,154,37...|[8.86131034783219...|\n",
      "| 3506|gambill kendrick ...|[gambill, kendric...|(20000,[1411,3206...|[9.48341757930206...|\n",
      "| 3764|australia upgrade...|[australia, upgra...|(20000,[7,183,958...|[7.60282216545510...|\n",
      "| 4590|airline clears ai...|[airline, clears,...|(20000,[195,2145,...|[0.25630569230180...|\n",
      "| 4823|at least 11 kille...|[least, killed, i...|(20000,[24,1126,1...|[0.00126140886826...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saving the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gs://bucket_name/LDAModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
