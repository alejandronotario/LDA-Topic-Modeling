{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA TOPIC MODELING\n",
    "\n",
    "This notebook applies LDA modeling to a dataset of news headlines:\n",
    "\n",
    "https://www.kaggle.com/therohk/million-headlines\n",
    "\n",
    "run at Dataproc with pySpark:\n",
    "\n",
    "https://cloud.google.com/dataproc/\n",
    "\n",
    "and following this repository:\n",
    "\n",
    "https://github.com/matthiasradtke/topic-modeling-codecentric-blog/blob/master/topicModeling-ccBlog.ipynb\n",
    "\n",
    "and Apache Spark Documentation:\n",
    "\n",
    "- http://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.clustering.LDA\n",
    "\n",
    "- https://spark.apache.org/docs/2.1.0/ml-clustering.html#latent-dirichlet-allocation-lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf, col, date_format\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, IDF, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "from pyspark.ml import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style(\"whitegrid\")\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.166.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"gs://ei-db/abcnews-date-text.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of news headlines:  1103665\n"
     ]
    }
   ],
   "source": [
    "print('Number of news headlines: ', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aba decides against community broadcasting licence'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw text of the first entry \n",
    "df.select('headline_text').take(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First transform dataframe headlines column to RDD to index it to work with__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df.rdd.map(lambda x: x['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding index to data dataframe\n",
    "headlines=texts.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "data = sqlContext.createDataFrame(headlines, [\"headlines\",'index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalize and tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "removePunct = udf(\n",
    "    lambda s: re.sub(r'[^a-zA-Z0-9]|[0-9]', r' ', s).strip().lower(), T.StringType())\n",
    "\n",
    "# normalize the post content (remove html tags, punctuation and lower case..)\n",
    "data_norm = data.withColumn(\"text\", removePunct(data.headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize \n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\",\n",
    "                           gaps=True, pattern=r'\\s+', minTokenLength=2)\n",
    "df_tokens = tokenizer.transform(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[aba, decides, ag...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[act, fire, witne...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, for, infr...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[air, nz, staff, ...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[air, nz, strike,...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, is, l...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, to, c...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[big, hopes, for,...|\n",
      "|big plan to boost...|   14|big plan to boost...|[big, plan, to, b...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing stopwords__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')#must be downloaded to run\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeStop=udf(lambda word: [x for x in word if x not in stopwords])\n",
    "df_tokens=df_tokens.withColumn('noStopWords',removeStop(df_tokens['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|         noStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[aba, decides, ag...|[aba, decides, co...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[act, fire, witne...|[act, fire, witne...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, for, infr...|[calls, infrastru...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[air, nz, staff, ...|[air, nz, staff, ...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[air, nz, strike,...|[air, nz, strike,...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, is, l...|[australia, locke...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, to, c...|[australia, contr...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[big, hopes, for,...|[big, hopes, laun...|\n",
      "|big plan to boost...|   14|big plan to boost...|[big, plan, to, b...|[big, plan, boost...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- headlines: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- noStopWords: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Changing noStopWords data type__\n",
    "\n",
    "It is necessary to change the column data type to run the count vectorizer. It must be T.ArrayType(T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_udf = udf(lambda x: x, T.ArrayType(T.StringType())) \n",
    "\n",
    "df_tokens=df_tokens.withColumn('final_words',label_udf(df_tokens.noStopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- headlines: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- noStopWords: string (nullable = true)\n",
      " |-- final_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|           headlines|index|                text|               words|         noStopWords|         final_words|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|aba decides again...|    0|aba decides again...|[aba, decides, ag...|[aba, decides, co...|[aba, decides, co...|\n",
      "|act fire witnesse...|    1|act fire witnesse...|[act, fire, witne...|[act, fire, witne...|[act, fire, witne...|\n",
      "|a g calls for inf...|    2|a g calls for inf...|[calls, for, infr...|[calls, infrastru...|[calls, infrastru...|\n",
      "|air nz staff in a...|    3|air nz staff in a...|[air, nz, staff, ...|[air, nz, staff, ...|[air, nz, staff, ...|\n",
      "|air nz strike to ...|    4|air nz strike to ...|[air, nz, strike,...|[air, nz, strike,...|[air, nz, strike,...|\n",
      "|ambitious olsson ...|    5|ambitious olsson ...|[ambitious, olsso...|[ambitious, olsso...|[ambitious, olsso...|\n",
      "|antic delighted w...|    6|antic delighted w...|[antic, delighted...|[antic, delighted...|[antic, delighted...|\n",
      "|aussie qualifier ...|    7|aussie qualifier ...|[aussie, qualifie...|[aussie, qualifie...|[aussie, qualifie...|\n",
      "|aust addresses un...|    8|aust addresses un...|[aust, addresses,...|[aust, addresses,...|[aust, addresses,...|\n",
      "|australia is lock...|    9|australia is lock...|[australia, is, l...|[australia, locke...|[australia, locke...|\n",
      "|australia to cont...|   10|australia to cont...|[australia, to, c...|[australia, contr...|[australia, contr...|\n",
      "|barca take record...|   11|barca take record...|[barca, take, rec...|[barca, take, rec...|[barca, take, rec...|\n",
      "|bathhouse plans m...|   12|bathhouse plans m...|[bathhouse, plans...|[bathhouse, plans...|[bathhouse, plans...|\n",
      "|big hopes for lau...|   13|big hopes for lau...|[big, hopes, for,...|[big, hopes, laun...|[big, hopes, laun...|\n",
      "|big plan to boost...|   14|big plan to boost...|[big, plan, to, b...|[big, plan, boost...|[big, plan, boost...|\n",
      "|blizzard buries u...|   15|blizzard buries u...|[blizzard, buries...|[blizzard, buries...|[blizzard, buries...|\n",
      "|brigadier dismiss...|   16|brigadier dismiss...|[brigadier, dismi...|[brigadier, dismi...|[brigadier, dismi...|\n",
      "|british combat tr...|   17|british combat tr...|[british, combat,...|[british, combat,...|[british, combat,...|\n",
      "|bryant leads lake...|   18|bryant leads lake...|[bryant, leads, l...|[bryant, leads, l...|[bryant, leads, l...|\n",
      "|bushfire victims ...|   19|bushfire victims ...|[bushfire, victim...|[bushfire, victim...|[bushfire, victim...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TF-IDF Matrix__\n",
    "\n",
    "Transforming the rdd into a DataFrame which has two columns — one has index and the other the list of words. CountVectorizer takes this data and returns a sparse matrix of term frequencies attached to the original Dataframe. Same thing goes for the IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF\n",
    "#VocabSize of 20000 words and words with frequencies above 10\n",
    "cv = CountVectorizer(inputCol=\"final_words\", outputCol=\"raw_features\",vocabSize=20000, minDF=10.0)\n",
    "cvmodel = cv.fit(df_tokens)\n",
    "\n",
    "result_cv = cvmodel.transform(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(result_cv)\n",
    "result_tfidf = idfModel.transform(result_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing documents:  883474 220191\n",
      "CPU times: user 180 ms, sys: 104 ms, total: 284 ms\n",
      "Wall time: 26min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#split 80% train set and 20% test set\n",
    "df_training, df_testing = result_tfidf.randomSplit([0.8, 0.2], 1)\n",
    "print('Training and testing documents: ', df_training.count(), df_testing.count())\n",
    "\n",
    "num_topics=30\n",
    "max_iterations=50\n",
    "lda = LDA(k=num_topics, maxIter=max_iterations)\n",
    "ldaModel = lda.fit(result_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Perplexity__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on testing and training data: inf,inf\n"
     ]
    }
   ],
   "source": [
    "lpt, lp = ldaModel.logPerplexity(df_testing), ldaModel.logPerplexity(df_training)\n",
    "print(\"Perplexity on testing and training data: \" + str(lp) + ',' + str(lpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|words                                           |\n",
      "+------------------------------------------------+\n",
      "|[drug, test, four, rejects, faces]              |\n",
      "|[work, queensland, budget, green, federal]      |\n",
      "|[industry, hits, crisis, considers, reports]    |\n",
      "|[changes, concerns, premier, rates, park]       |\n",
      "|[bid, media, safety, victims, fears]            |\n",
      "|[killed, strike, tour, land, family]            |\n",
      "|[man, charged, murder, missing, woman]          |\n",
      "|[funding, future, move, abbott, japan]          |\n",
      "|[rural, national, workers, opposition, business]|\n",
      "|[north, face, charges, probe, rise]             |\n",
      "|[security, urges, oil, food, close]             |\n",
      "|[day, farmers, one, record, inquiry]            |\n",
      "|[claims, darwin, murray, told, long]            |\n",
      "|[health, dies, car, perth, assault]             |\n",
      "|[coast, boost, gold, deal, get]                 |\n",
      "|[abc, public, arrested, control, energy]        |\n",
      "|[home, open, return, season, youth]             |\n",
      "|[attack, school, child, sex, indigenous]        |\n",
      "|[melbourne, brisbane, city, aussie, rudd]       |\n",
      "|[ban, decision, wont, labor, meet]              |\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------------------------------+\n",
      "|weights                                 |\n",
      "+----------------------------------------+\n",
      "|[0.0216, 0.0203, 0.0159, 0.0155, 0.0152]|\n",
      "|[0.0228, 0.0198, 0.0182, 0.0166, 0.0165]|\n",
      "|[0.0202, 0.0148, 0.0143, 0.0142, 0.0116]|\n",
      "|[0.0196, 0.0195, 0.0143, 0.0142, 0.0139]|\n",
      "|[0.0169, 0.0148, 0.0145, 0.0143, 0.0141]|\n",
      "|[0.0166, 0.0146, 0.0142, 0.0140, 0.0137]|\n",
      "|[0.0398, 0.0278, 0.0264, 0.0226, 0.0190]|\n",
      "|[0.0258, 0.0191, 0.0138, 0.0135, 0.0132]|\n",
      "|[0.0257, 0.0216, 0.0184, 0.0182, 0.0162]|\n",
      "|[0.0202, 0.0181, 0.0181, 0.0168, 0.0162]|\n",
      "|[0.0173, 0.0162, 0.0137, 0.0135, 0.0134]|\n",
      "|[0.0276, 0.0247, 0.0240, 0.0195, 0.0190]|\n",
      "|[0.0241, 0.0173, 0.0142, 0.0136, 0.0133]|\n",
      "|[0.0264, 0.0251, 0.0232, 0.0192, 0.0173]|\n",
      "|[0.0290, 0.0253, 0.0222, 0.0202, 0.0184]|\n",
      "|[0.0244, 0.0213, 0.0187, 0.0115, 0.0115]|\n",
      "|[0.0267, 0.0236, 0.0195, 0.0175, 0.0135]|\n",
      "|[0.0241, 0.0221, 0.0212, 0.0212, 0.0198]|\n",
      "|[0.0230, 0.0162, 0.0144, 0.0137, 0.0133]|\n",
      "|[0.0208, 0.0159, 0.0157, 0.0139, 0.0136]|\n",
      "+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print topics and top-weighted terms\n",
    "topics = ldaModel.describeTopics(maxTermsPerTopic=5)\n",
    "vocabArray = cvmodel.vocabulary\n",
    "numTopics=20\n",
    "\n",
    "ListOfIndexToWords = udf(lambda wl: list([vocabArray[w] for w in wl]))\n",
    "FormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n",
    "\n",
    "topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)\n",
    "topics.select(FormatNumbers(topics.termWeights).alias('weights')).show(truncate=False, n=numTopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Top Topics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------+----------------------------------------+\n",
      "|topic|words                                           |weights                                 |\n",
      "+-----+------------------------------------------------+----------------------------------------+\n",
      "|1    |[drug, test, four, rejects, faces]              |[0.0216, 0.0203, 0.0159, 0.0155, 0.0152]|\n",
      "|2    |[work, queensland, budget, green, federal]      |[0.0228, 0.0198, 0.0182, 0.0166, 0.0165]|\n",
      "|3    |[industry, hits, crisis, considers, reports]    |[0.0202, 0.0148, 0.0143, 0.0142, 0.0116]|\n",
      "|4    |[changes, concerns, premier, rates, park]       |[0.0196, 0.0195, 0.0143, 0.0142, 0.0139]|\n",
      "|5    |[bid, media, safety, victims, fears]            |[0.0169, 0.0148, 0.0145, 0.0143, 0.0141]|\n",
      "|6    |[killed, strike, tour, land, family]            |[0.0166, 0.0146, 0.0142, 0.0140, 0.0137]|\n",
      "|7    |[man, charged, murder, missing, woman]          |[0.0398, 0.0278, 0.0264, 0.0226, 0.0190]|\n",
      "|8    |[funding, future, move, abbott, japan]          |[0.0258, 0.0191, 0.0138, 0.0135, 0.0132]|\n",
      "|9    |[rural, national, workers, opposition, business]|[0.0257, 0.0216, 0.0184, 0.0182, 0.0162]|\n",
      "|10   |[north, face, charges, probe, rise]             |[0.0202, 0.0181, 0.0181, 0.0168, 0.0162]|\n",
      "|11   |[security, urges, oil, food, close]             |[0.0173, 0.0162, 0.0137, 0.0135, 0.0134]|\n",
      "|12   |[day, farmers, one, record, inquiry]            |[0.0276, 0.0247, 0.0240, 0.0195, 0.0190]|\n",
      "|13   |[claims, darwin, murray, told, long]            |[0.0241, 0.0173, 0.0142, 0.0136, 0.0133]|\n",
      "|14   |[health, dies, car, perth, assault]             |[0.0264, 0.0251, 0.0232, 0.0192, 0.0173]|\n",
      "|15   |[coast, boost, gold, deal, get]                 |[0.0290, 0.0253, 0.0222, 0.0202, 0.0184]|\n",
      "|16   |[abc, public, arrested, control, energy]        |[0.0244, 0.0213, 0.0187, 0.0115, 0.0115]|\n",
      "|17   |[home, open, return, season, youth]             |[0.0267, 0.0236, 0.0195, 0.0175, 0.0135]|\n",
      "|18   |[attack, school, child, sex, indigenous]        |[0.0241, 0.0221, 0.0212, 0.0212, 0.0198]|\n",
      "|19   |[melbourne, brisbane, city, aussie, rudd]       |[0.0230, 0.0162, 0.0144, 0.0137, 0.0133]|\n",
      "|20   |[ban, decision, wont, labor, meet]              |[0.0208, 0.0159, 0.0157, 0.0139, 0.0136]|\n",
      "+-----+------------------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Topics: 20 Vocabulary: 20000\n"
     ]
    }
   ],
   "source": [
    "toptopics = topics.select((topics.topic + 1).alias('topic'),\n",
    "                          ListOfIndexToWords(topics.termIndices).alias('words'),\n",
    "                          FormatNumbers(topics.termWeights).alias('weights'))\n",
    "toptopics.show(truncate=False, n=numTopics)\n",
    "print('Topics:', numTopics, 'Vocabulary:', len(vocabArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldaModel.isDistributed())\n",
    "path = \"gs://bucket_name/LDAModel/\"\n",
    "\n",
    "model_number = 'x'\n",
    "cvModel.save(path + 'CVModel'+ model_number)\n",
    "ldaModel.save(path + 'LDAModel'+ model_number)\n",
    "lda.save(path + 'LDA_'+ model_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Details of some topics__\n",
    "\n",
    "In this case 5 topics. It can be displayed all the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "\n",
      "Top Document(s): [Row(ID=801645, weight=0.9782088377532229)]\n",
      "+-----------------------------------------+\n",
      "|headlines                                |\n",
      "+-----------------------------------------+\n",
      "|jim maxwell previews the first ashes test|\n",
      "+-----------------------------------------+\n",
      "\n",
      "Top terms:\n",
      "[drug, test, four, rejects, faces] \n",
      "\n",
      "===================================================\n",
      "Topic 1\n",
      "\n",
      "Top Document(s): [Row(ID=654258, weight=0.9736804583104055)]\n",
      "+--------------------------------------+\n",
      "|headlines                             |\n",
      "+--------------------------------------+\n",
      "|light rail work on track officials say|\n",
      "+--------------------------------------+\n",
      "\n",
      "Top terms:\n",
      "[work, queensland, budget, green, federal] \n",
      "\n",
      "===================================================\n",
      "Topic 2\n",
      "\n",
      "Top Document(s): [Row(ID=54316, weight=0.9741522565136251)]\n",
      "+--------------------------------+\n",
      "|headlines                       |\n",
      "+--------------------------------+\n",
      "|ganguly scoffs at scheduling row|\n",
      "+--------------------------------+\n",
      "\n",
      "Top terms:\n",
      "[industry, hits, crisis, considers, reports] \n",
      "\n",
      "===================================================\n",
      "Topic 3\n",
      "\n",
      "Top Document(s): [Row(ID=1023117, weight=0.982330181953936)]\n",
      "+--------------------------------------------------+\n",
      "|headlines                                         |\n",
      "+--------------------------------------------------+\n",
      "|nsw to refund 10 cents for cans and containers cds|\n",
      "+--------------------------------------------------+\n",
      "\n",
      "Top terms:\n",
      "[changes, concerns, premier, rates, park] \n",
      "\n",
      "===================================================\n",
      "Topic 4\n",
      "\n",
      "Top Document(s): [Row(ID=426766, weight=0.9783422806513288)]\n",
      "+----------------------------------------------+\n",
      "|headlines                                     |\n",
      "+----------------------------------------------+\n",
      "|recycled water still dependent on 40pc trigger|\n",
      "+----------------------------------------------+\n",
      "\n",
      "Top terms:\n",
      "[bid, media, safety, victims, fears] \n",
      "\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "df = ldaModel.transform(result_tfidf) #defining the dataframe\n",
    "numTopics_toShow=5 #\n",
    "topWords = topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).take(numTopics_toShow)\n",
    "\n",
    "nTopDoc = 1  # show single top document (1) or also second (2) ..\n",
    "\n",
    "for i in range(0, numTopics_toShow):\n",
    "    ntopic = i  # which topic \n",
    "    print('Topic ' + str(ntopic) + '\\n')  \n",
    "\n",
    "    df_sliced = df.select(\"index\", \"topicDistribution\") \\\n",
    "        .rdd.map(lambda r: Row(ID=int(r[0]), weight=float(r[1][ntopic]))).toDF()\n",
    "\n",
    "    DocIDs = df_sliced.sort(df_sliced.weight.desc()).take(nTopDoc)\n",
    "    print('Top Document(s):',DocIDs)\n",
    "    for d_id in DocIDs:\n",
    "        df_tokens.filter(df_tokens.index == d_id[0]) \\\n",
    "            .select('headlines') \\\n",
    "            .show(truncate=False)\n",
    "\n",
    "    print('Top terms:')\n",
    "    print(topWords[ntopic][0], '\\n')\n",
    "    print('===================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Details of a single topic__\n",
    "\n",
    "It includes the top topic or second, third, etc., in this case second form example, and the x top terms, in this case 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677619\n",
      "Topic 3\n",
      "\n",
      "Second Document: [Row(ID=426766, weight=0.9783422806513288)]\n",
      "+-----------------------------------------------+\n",
      "|headlines                                      |\n",
      "+-----------------------------------------------+\n",
      "|rba holds fire on rates rba holds fire on rates|\n",
      "+-----------------------------------------------+\n",
      "\n",
      "Top 5 terms:\n",
      "[changes, concerns, premier, rates, park] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the top documents according to lda for topic number ntopic\n",
    "ntopic = 3 # number of topic\n",
    "nTopDoc = 2 # show top document (1) or second (2) ...\n",
    "\n",
    "df_sliced = df.select(\"index\",\"topicDistribution\").rdd.map(lambda r: Row(ID = int(r[0]),\n",
    "                                                                         sliced = float(r[1][ntopic]))).toDF()\n",
    "topDocID = df_sliced.sort(df_sliced.sliced.desc()).take(nTopDoc)[nTopDoc-1][0]\n",
    "print(topDocID)\n",
    "print('Topic ' + str(ntopic) +'\\n')\n",
    "\n",
    "print('Second Document:',DocIDs)\n",
    "df_tokens.filter(df_tokens.index == topDocID)\\\n",
    "    .select('headlines')\\\n",
    "    .show(truncate = False)\n",
    "\n",
    "print('Top 5 terms:')\n",
    "print(topics.select(ListOfIndexToWords(topics.termIndices).alias('words')).take(numTopics)[ntopic][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the top documents result__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "countTopDocs = df.select('topicDistribution')\\\n",
    "                .rdd.map(lambda r: Row( nTopTopic = int(np.argmax(r)))).toDF() \\\n",
    "                .groupBy(\"nTopTopic\").count().sort(\"nTopTopic\")\n",
    "\n",
    "pdf = countTopDocs.toPandas()\n",
    "pdfLess = pdf.drop(pdf.index[[1,3,7,8,10,11,14,15]]).reset_index()\n",
    "\n",
    "pdfLess.plot(color = '#44D3A5', legend = False,\n",
    "                           kind = 'bar', use_index = True, y = 'count', grid = False)\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predicting a single document__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.001), (2, 0.001), (3, 0.001), (4, 0.219), (5, 0.095), (6, 0.001), (7, 0.001), (8, 0.001), (9, 0.001), (10, 0.001), (11, 0.001), (12, 0.001), (13, 0.001), (14, 0.448), (15, 0.001), (16, 0.001), (17, 0.001), (18, 0.001), (19, 0.001), (20, 0.001), (21, 0.001), (22, 0.001), (23, 0.001), (24, 0.001), (25, 0.001), (26, 0.001), (27, 0.215), (28, 0.001), (29, 0.001), (30, 0.001)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(headlines='call for drip feed baby bonus')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexPost = 0\n",
    "idd = 99999 #For example\n",
    "\n",
    "testTopicDistributions = ldaModel.transform(result_tfidf)\n",
    "\n",
    "alist = testTopicDistributions.filter(testTopicDistributions.\n",
    "                                      index == idd).select(\"topicDistribution\").collect()[0][0]\n",
    "print([ (index+1,float(\"{0:.3f}\".format(item)) ) for (index,item) in enumerate( alist ) ])\n",
    "\n",
    "df_tokens.filter(df_tokens.index == idd).select(\"headlines\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
